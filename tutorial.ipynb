{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial on using the training pipeline for the event-based eye tracking challenge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse, json, os, mlflow\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from model.BaselineEyeTrackingModel import CNN_GRU\n",
    "from utils.training_utils import train_epoch, validate_epoch, top_k_checkpoints\n",
    "from utils.metrics import weighted_MSELoss\n",
    "from dataset import ThreeETplus_Eyetracking, ScaleLabel, NormalizeLabel, \\\n",
    "    LabelTemporalSubsample, NormalizeLabel, SliceLongEventsToShort, \\\n",
    "    EventSlicesToVoxelGrid, SliceByTimeEventsTargets\n",
    "import tonic.transforms as transforms\n",
    "from tonic import SlicedDataset, DiskCachedDataset\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Examplar config file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_file = 'train_baseline.json'\n",
    "with open(os.path.join('./configs', config_file), 'r') as f:\n",
    "    config = json.load(f)\n",
    "args = argparse.Namespace(**config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup mlflow tracking server (local)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='file:///C:/Users/Junkyy/CV_exercise/thesis/3et_challenge_2025-main/mlruns/617056576973668955', creation_time=1745660511476, experiment_id='617056576973668955', last_update_time=1745660511476, lifecycle_stage='active', name='trial_experiment', tags={}>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlflow.set_tracking_uri(args.mlflow_path)\n",
    "mlflow.set_experiment(experiment_name=args.experiment_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model and Optimizer Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your model, optimizer, and criterion\n",
    "model = eval(args.architecture)(args).to(args.device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=args.lr)\n",
    "\n",
    "if args.loss == \"mse\":\n",
    "    criterion = nn.MSELoss()\n",
    "elif args.loss == \"weighted_mse\":\n",
    "    criterion = weighted_MSELoss(weights=torch.tensor((args.sensor_width/args.sensor_height, 1)).to(args.device), \\\n",
    "                                    reduction='mean')\n",
    "else:\n",
    "    raise ValueError(\"Invalid loss name\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloding and Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we define the label transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "factor = args.spatial_factor # spatial downsample factor\n",
    "temp_subsample_factor = args.temporal_subsample_factor # downsampling original 100Hz label to 20Hz\n",
    "\n",
    "# The original labels are spatially downsampled with 'factor', downsampled to 20Hz, and normalized w.r.t width and height to [0,1]\n",
    "label_transform = transforms.Compose([\n",
    "    ScaleLabel(factor),\n",
    "    LabelTemporalSubsample(temp_subsample_factor),\n",
    "    NormalizeLabel(pseudo_width=640*factor, pseudo_height=480*factor)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we define the raw event recording and label dataset, the raw events spatial coordinates are also spatially downsampled to 80x60 spatial resolutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_orig = ThreeETplus_Eyetracking(save_to=args.data_dir, split=\"train\", \\\n",
    "                transform=transforms.Downsample(spatial_factor=factor), \n",
    "                target_transform=label_transform)\n",
    "val_data_orig = ThreeETplus_Eyetracking(save_to=args.data_dir, split=\"val\", \\\n",
    "                transform=transforms.Downsample(spatial_factor=factor),\n",
    "                target_transform=label_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we slice the event recordings into sub-sequences. The time-window is determined by the sequence length (train_length, val_length) and the temporal subsample factor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "slicing_time_window = args.train_length*int(10000/temp_subsample_factor) #microseconds\n",
    "train_stride_time = int(10000/temp_subsample_factor*args.train_stride) #microseconds\n",
    "\n",
    "train_slicer=SliceByTimeEventsTargets(slicing_time_window, overlap=slicing_time_window-train_stride_time, \\\n",
    "                seq_length=args.train_length, seq_stride=args.train_stride, include_incomplete=False)\n",
    "# the validation set is sliced to non-overlapping sequences\n",
    "val_slicer=SliceByTimeEventsTargets(slicing_time_window, overlap=0, \\\n",
    "                seq_length=args.val_length, seq_stride=args.val_stride, include_incomplete=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After slicing the raw event recordings into sub-sequences, we make each subsequences into your favorite event representation, in this case event voxel-\n",
    "\n",
    "You could also try other representations with the Tonic library easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_slicer_transform = transforms.Compose([\n",
    "    SliceLongEventsToShort(time_window=int(10000/temp_subsample_factor), overlap=0, include_incomplete=True),\n",
    "    EventSlicesToVoxelGrid(sensor_size=(int(640*factor), int(480*factor), 2), \\\n",
    "                            n_time_bins=args.n_time_bins, per_channel_normalize=args.voxel_grid_ch_normaization)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the Tonic SlicedDataset class to handle the collation of the sub-sequences into batches.\n",
    "\n",
    "The slicing indices will be cached to disk for faster slicing in the future, for the same slice parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata read from ./metadata/3et_train_tl_30_ts15_ch3\\slice_metadata.h5.\n",
      "Metadata read from ./metadata/3et_val_vl_30_vs30_ch3\\slice_metadata.h5.\n"
     ]
    }
   ],
   "source": [
    "train_data = SlicedDataset(train_data_orig, train_slicer, transform=post_slicer_transform, metadata_path=f\"./metadata/3et_train_tl_{args.train_length}_ts{args.train_stride}_ch{args.n_time_bins}\")\n",
    "val_data = SlicedDataset(val_data_orig, val_slicer, transform=post_slicer_transform, metadata_path=f\"./metadata/3et_val_vl_{args.val_length}_vs{args.val_stride}_ch{args.n_time_bins}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cache the preprocessed data to disk to speed up training. The first epoch will be slow, but the following epochs will be fast. This will consume certain disk space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = DiskCachedDataset(train_data, cache_path=f'./cached_dataset/train_tl_{args.train_length}_ts{args.train_stride}_ch{args.n_time_bins}')\n",
    "val_data = DiskCachedDataset(val_data, cache_path=f'./cached_dataset/val_vl_{args.val_length}_vs{args.val_stride}_ch{args.n_time_bins}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we wrap the dataset with pytorch dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_data, batch_size=args.batch_size, shuffle=True, \\\n",
    "                            num_workers=int(os.cpu_count()-2), pin_memory=True)\n",
    "val_loader = DataLoader(val_data, batch_size=args.batch_size, shuffle=False, \\\n",
    "                        num_workers=int(os.cpu_count()-2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Training Loop Functionalities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, val_loader, criterion, optimizer, args):\n",
    "    best_val_loss = float(\"inf\")\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(args.num_epochs):\n",
    "        # Wrap train_loader with tqdm for progress bar\n",
    "        train_pbar = tqdm(train_loader, desc=f\"Training Epoch {epoch+1}/{args.num_epochs}\")\n",
    "        model, train_loss, metrics = train_epoch(model, train_pbar, criterion, optimizer, args)\n",
    "        mlflow.log_metric(\"train_loss\", train_loss, step=epoch)\n",
    "        mlflow.log_metrics(metrics['tr_p_acc_all'], step=epoch)\n",
    "        mlflow.log_metrics(metrics['tr_p_euc_error_all'], step=epoch)\n",
    "\n",
    "        if args.val_interval > 0 and (epoch + 1) % args.val_interval == 0:\n",
    "            # Wrap val_loader with tqdm for progress bar\n",
    "            val_pbar = tqdm(val_loader, desc=f\"Validation Epoch {epoch+1}/{args.num_epochs}\")\n",
    "            val_loss, val_metrics = validate_epoch(model, val_pbar, criterion, args)\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                # save the new best model to MLflow artifact\n",
    "                torch.save(model.state_dict(), os.path.join(mlflow.get_artifact_uri(), \\\n",
    "                            f\"model_best_ep{epoch}_val_loss_{val_loss:.4f}.pth\"))\n",
    "                \n",
    "                # Keep only top K checkpoints\n",
    "                top_k_checkpoints(args, mlflow.get_artifact_uri())\n",
    "                \n",
    "            print(f\"[Validation] at Epoch {epoch+1}/{args.num_epochs}: Val Loss: {val_loss:.4f}\")\n",
    "            mlflow.log_metric(\"val_loss\", val_loss, step=epoch)\n",
    "            mlflow.log_metrics(val_metrics['val_p_acc_all'], step=epoch)\n",
    "            mlflow.log_metrics(val_metrics['val_p_euc_error_all'], step=epoch)\n",
    "        # Print progress\n",
    "        print(f\"Epoch {epoch+1}/{args.num_epochs}: Train Loss: {train_loss:.4f}\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, val_loader, criterion, optimizer, args):\n",
    "    best_val_loss = float(\"inf\")\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(args.num_epochs):\n",
    "        # Wrap train_loader with tqdm for progress bar\n",
    "        train_pbar = tqdm(train_loader, desc=f\"Training Epoch {epoch+1}/{args.num_epochs}\")\n",
    "        model, train_loss, metrics = train_epoch(model, train_pbar, criterion, optimizer, args)\n",
    "        mlflow.log_metric(\"train_loss\", train_loss, step=epoch)\n",
    "        mlflow.log_metrics(metrics['tr_p_acc_all'], step=epoch)\n",
    "        mlflow.log_metrics(metrics['tr_p_euc_error_all'], step=epoch)\n",
    "\n",
    "        if args.val_interval > 0 and (epoch + 1) % args.val_interval == 0:\n",
    "            # Wrap val_loader with tqdm for progress bar\n",
    "            val_pbar = tqdm(val_loader, desc=f\"Validation Epoch {epoch+1}/{args.num_epochs}\")\n",
    "            val_loss, val_metrics = validate_epoch(model, val_pbar, criterion, args)\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                # save the new best model to MLflow artifact\n",
    "\n",
    "                # Save to a local file with unique name\n",
    "                model_filename = f\"model_best_ep{epoch}_val_loss_{val_loss:.4f}.pth\"\n",
    "                torch.save(model.state_dict(), model_filename)\n",
    "                # Log it to MLflow\n",
    "                mlflow.log_artifact(model_filename)\n",
    "\n",
    "                # Keep only top K checkpoints\n",
    "                top_k_checkpoints(args, mlflow.get_artifact_uri())\n",
    "                \n",
    "            print(f\"[Validation] at Epoch {epoch+1}/{args.num_epochs}: Val Loss: {val_loss:.4f}\")\n",
    "            mlflow.log_metric(\"val_loss\", val_loss, step=epoch)\n",
    "            mlflow.log_metrics(val_metrics['val_p_acc_all'], step=epoch)\n",
    "            mlflow.log_metrics(val_metrics['val_p_euc_error_all'], step=epoch)\n",
    "        # Print progress\n",
    "        print(f\"Epoch {epoch+1}/{args.num_epochs}: Train Loss: {train_loss:.4f}\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import urlparse\n",
    "import os\n",
    "\n",
    "def top_k_checkpoints(args, artifact_uri):\n",
    "    # Convert artifact URI to local path\n",
    "    parsed_uri = urlparse(artifact_uri)\n",
    "    artifact_path = parsed_uri.path\n",
    "    if os.name == 'nt' and artifact_path.startswith('/'):\n",
    "        artifact_path = artifact_path[1:]\n",
    "\n",
    "    # List all .pth files in artifact directory\n",
    "    model_checkpoints = [f for f in os.listdir(artifact_path) if f.endswith(\".pth\")]\n",
    "\n",
    "    # Keep only top-K based on val loss embedded in filename\n",
    "    if len(model_checkpoints) > args.save_k_best:\n",
    "        # Sort based on validation loss parsed from filename\n",
    "        model_checkpoints.sort(key=lambda name: float(name.split(\"val_loss_\")[1].replace(\".pth\", \"\")))\n",
    "        for ckpt_to_remove in model_checkpoints[args.save_k_best:]:\n",
    "            os.remove(os.path.join(artifact_path, ckpt_to_remove))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Training!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the major training loop including validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start MLflow run\n",
    "with mlflow.start_run(run_name=args.run_name):\n",
    "    # dump this training file to MLflow artifact\n",
    "    # mlflow.log_artifact(__file__) # Disabled for notebook, it is included in with the script\n",
    "\n",
    "    # Log all hyperparameters to MLflow\n",
    "    mlflow.log_params(vars(args))\n",
    "    # also dump the args to a JSON file in MLflow artifact\n",
    "    with open(os.path.join(mlflow.get_artifact_uri(), \"args.json\"), 'w') as f:\n",
    "        json.dump(vars(args), f)\n",
    "\n",
    "    # Train your model\n",
    "    model = train(model, train_loader, val_loader, criterion, optimizer, args)\n",
    "\n",
    "    # Save your model for the last epoch\n",
    "    torch.save(model.state_dict(), os.path.join(mlflow.get_artifact_uri(), f\"model_last_epoch{args.num_epochs}.pth\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/05/01 12:08:03 WARNING mlflow.utils.git_utils: Failed to import Git (the Git executable is probably not on your PATH), so Git SHA is not available. Error: Failed to initialize: Bad git executable.\n",
      "The git executable must be specified in one of the following ways:\n",
      "    - be included in your $PATH\n",
      "    - be set via $GIT_PYTHON_GIT_EXECUTABLE\n",
      "    - explicitly set via git.refresh(<full-path-to-git-executable>)\n",
      "\n",
      "All git commands will error until this is rectified.\n",
      "\n",
      "This initial message can be silenced or aggravated in the future by setting the\n",
      "$GIT_PYTHON_REFRESH environment variable. Use one of the following values:\n",
      "    - quiet|q|silence|s|silent|none|n|0: for no message or exception\n",
      "    - warn|w|warning|log|l|1: for a warning message (logging level CRITICAL, displayed by default)\n",
      "    - error|e|exception|raise|r|2: for a raised exception\n",
      "\n",
      "Example:\n",
      "    export GIT_PYTHON_REFRESH=quiet\n",
      "\n",
      "Training Epoch 1/20: 100%|██████████| 81/81 [11:07<00:00,  8.24s/it, loss=0.0181] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20: Train Loss: 0.1559\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2/20: 100%|██████████| 81/81 [09:18<00:00,  6.89s/it, loss=0.00844]\n",
      "Validation Epoch 2/20: 100%|██████████| 15/15 [00:43<00:00,  2.88s/it, loss=0.00672]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Validation] at Epoch 2/20: Val Loss: 0.0140\n",
      "Epoch 2/20: Train Loss: 0.0088\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3/20: 100%|██████████| 81/81 [09:12<00:00,  6.82s/it, loss=0.00696]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/20: Train Loss: 0.0074\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4/20: 100%|██████████| 81/81 [09:15<00:00,  6.85s/it, loss=0.00324]\n",
      "Validation Epoch 4/20: 100%|██████████| 15/15 [00:45<00:00,  3.03s/it, loss=0.014]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Validation] at Epoch 4/20: Val Loss: 0.0145\n",
      "Epoch 4/20: Train Loss: 0.0063\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5/20: 100%|██████████| 81/81 [08:39<00:00,  6.42s/it, loss=0.0078] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/20: Train Loss: 0.0054\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 6/20: 100%|██████████| 81/81 [09:35<00:00,  7.10s/it, loss=0.00452]\n",
      "Validation Epoch 6/20: 100%|██████████| 15/15 [00:46<00:00,  3.11s/it, loss=0.00963]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Validation] at Epoch 6/20: Val Loss: 0.0145\n",
      "Epoch 6/20: Train Loss: 0.0051\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 7/20: 100%|██████████| 81/81 [09:23<00:00,  6.96s/it, loss=0.0024] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/20: Train Loss: 0.0045\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 8/20: 100%|██████████| 81/81 [09:00<00:00,  6.67s/it, loss=0.00215]\n",
      "Validation Epoch 8/20: 100%|██████████| 15/15 [00:42<00:00,  2.86s/it, loss=0.00916]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Validation] at Epoch 8/20: Val Loss: 0.0138\n",
      "Epoch 8/20: Train Loss: 0.0041\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 9/20: 100%|██████████| 81/81 [09:24<00:00,  6.97s/it, loss=0.00188]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/20: Train Loss: 0.0037\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 10/20: 100%|██████████| 81/81 [08:54<00:00,  6.60s/it, loss=0.00562]\n",
      "Validation Epoch 10/20: 100%|██████████| 15/15 [00:40<00:00,  2.67s/it, loss=0.00944]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Validation] at Epoch 10/20: Val Loss: 0.0145\n",
      "Epoch 10/20: Train Loss: 0.0035\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 11/20: 100%|██████████| 81/81 [08:32<00:00,  6.33s/it, loss=0.00459]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/20: Train Loss: 0.0034\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 12/20: 100%|██████████| 81/81 [08:48<00:00,  6.52s/it, loss=0.00236]\n",
      "Validation Epoch 12/20: 100%|██████████| 15/15 [00:41<00:00,  2.76s/it, loss=0.00728]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Validation] at Epoch 12/20: Val Loss: 0.0147\n",
      "Epoch 12/20: Train Loss: 0.0029\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 13/20: 100%|██████████| 81/81 [09:28<00:00,  7.02s/it, loss=0.00348]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/20: Train Loss: 0.0028\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 14/20: 100%|██████████| 81/81 [08:43<00:00,  6.46s/it, loss=0.00233]\n",
      "Validation Epoch 14/20: 100%|██████████| 15/15 [00:40<00:00,  2.70s/it, loss=0.0046] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Validation] at Epoch 14/20: Val Loss: 0.0142\n",
      "Epoch 14/20: Train Loss: 0.0025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 15/20: 100%|██████████| 81/81 [08:35<00:00,  6.36s/it, loss=0.00257]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/20: Train Loss: 0.0024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 16/20: 100%|██████████| 81/81 [08:31<00:00,  6.32s/it, loss=0.00166]\n",
      "Validation Epoch 16/20: 100%|██████████| 15/15 [00:40<00:00,  2.70s/it, loss=0.00536]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Validation] at Epoch 16/20: Val Loss: 0.0134\n",
      "Epoch 16/20: Train Loss: 0.0021\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 17/20: 100%|██████████| 81/81 [09:21<00:00,  6.93s/it, loss=0.00129]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/20: Train Loss: 0.0020\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 18/20: 100%|██████████| 81/81 [09:24<00:00,  6.96s/it, loss=0.00134]\n",
      "Validation Epoch 18/20: 100%|██████████| 15/15 [00:42<00:00,  2.83s/it, loss=0.00648]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Validation] at Epoch 18/20: Val Loss: 0.0145\n",
      "Epoch 18/20: Train Loss: 0.0019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 19/20: 100%|██████████| 81/81 [09:08<00:00,  6.77s/it, loss=0.00158]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/20: Train Loss: 0.0019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 20/20: 100%|██████████| 81/81 [09:21<00:00,  6.93s/it, loss=0.00129] \n",
      "Validation Epoch 20/20: 100%|██████████| 15/15 [00:42<00:00,  2.83s/it, loss=0.00507]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Validation] at Epoch 20/20: Val Loss: 0.0142\n",
      "Epoch 20/20: Train Loss: 0.0016\n"
     ]
    }
   ],
   "source": [
    "# Start MLflow run\n",
    "with mlflow.start_run(run_name=args.run_name):\n",
    "    # Log all hyperparameters to MLflow\n",
    "    mlflow.log_params(vars(args))\n",
    "\n",
    "    # Save args to a temporary file and log it as an artifact\n",
    "    args_path = \"args.json\"\n",
    "    with open(args_path, 'w') as f:\n",
    "        json.dump(vars(args), f)\n",
    "    mlflow.log_artifact(args_path)\n",
    "\n",
    "    # Train your model\n",
    "    model = train(model, train_loader, val_loader, criterion, optimizer, args)\n",
    "\n",
    "    # Save model state_dict to file and log it\n",
    "    model_path = f\"model_last_epoch{args.num_epochs}.pth\"\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "    mlflow.log_artifact(model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start MLflow run\n",
    "with mlflow.start_run(run_name=args.run_name):\n",
    "    # Log all hyperparameters to MLflow\n",
    "    mlflow.log_params(vars(args))\n",
    "\n",
    "    # Save args to a temporary file and log it as an artifact\n",
    "    args_path = \"args.json\"\n",
    "    with open(args_path, 'w') as f:\n",
    "        json.dump(vars(args), f)\n",
    "    mlflow.log_artifact(args_path)\n",
    "\n",
    "    # Train your model\n",
    "    model = train(model, train_loader, val_loader, criterion, optimizer, args)\n",
    "\n",
    "    # Save model state_dict to file and log it\n",
    "    model_path = f\"model_last_epoch{args.num_epochs}.pth\"\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "    mlflow.log_artifact(model_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
